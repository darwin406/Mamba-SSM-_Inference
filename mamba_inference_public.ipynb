{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is suitable for Mamba-7B. \n",
    "Huggingface ID: TRI-ML/mamba-7b-rw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\") \n",
    "device=torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at tri-ml/mamba-7b-rw were not used when initializing MambaForCausalLM: ['model.lm_head.weight']\n",
      "- This IS expected if you are initializing MambaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MambaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"tri-ml/mamba-7b-rw\").to(dtype=torch.bfloat16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "from mamba_ssm.utils.generation import sample\n",
    "try:\n",
    "    from mamba_ssm.ops.triton.selective_state_update import selective_state_update\n",
    "except ImportError:\n",
    "    selective_state_update = None\n",
    "try:\n",
    "    from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn\n",
    "except ImportError:\n",
    "    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None\n",
    "from io import BytesIO\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "import easydict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange, repeat\n",
    "from mamba_ssm.ops.selective_scan_interface import selective_scan_fn, mamba_inner_fn,selective_scan_ref\n",
    "try:\n",
    "    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n",
    "except ImportError:\n",
    "    causal_conv1d_fn, causal_conv1d_update = None\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Default settings for model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "model_weights = model.state_dict()\n",
    "model_param=\"7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "if model_param == \"2.8b\":\n",
    "    layer_num = 64\n",
    "    d_model = 2560\n",
    "elif model_param == \"130m\":\n",
    "    layer_num = 24\n",
    "    d_model = 768\n",
    "\n",
    "elif model_param == \"7b\":\n",
    "    layer_num=64\n",
    "    d_model=4096\n",
    "\n",
    "d_state = 16\n",
    "dt_rank = model_weights['backbone.layers.0.mixer.x_proj.weight'].shape[0] - d_state*2\n",
    "d_inner = d_model * 2\n",
    "d_conv=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# mixer_in_proj_weights\n",
    "mixer_in_proj_weights = ['backbone.layers.{0}.mixer.in_proj.weight'.format(i) for i in range(layer_num)]\n",
    "mixer_out_proj_weights = ['backbone.layers.{0}.mixer.out_proj.weight'.format(i) for i in range(layer_num)]\n",
    "mixer_A_log_weights=['backbone.layers.{0}.mixer.A_log'.format(i) for i in range(layer_num)]         \n",
    "mixer_conv1d_weights=['backbone.layers.{0}.mixer.conv1d.weight'.format(i) for i in range (layer_num)]\n",
    "mixer_conv1d_bias=['backbone.layers.{0}.mixer.conv1d.bias'.format(i) for i in range (layer_num)]\n",
    "mixer_dt_proj_weights=['backbone.layers.{0}.mixer.dt_proj.weight'.format(i) for i in range(layer_num)]\n",
    "mixer_dt_proj_bias=['backbone.layers.{0}.mixer.dt_proj.bias'.format(i) for i in range(layer_num)]\n",
    "mixer_x_proj_weights=['backbone.layers.{0}.mixer.x_proj.weight'.format(i) for i in range(layer_num)]\n",
    "norm_weights = ['backbone.layers.{0}.norm.weight'.format(i) for i in range(layer_num)]\n",
    "mixer_D = ['backbone.layers.{0}.mixer.D'.format(i) for i in range(layer_num)]\n",
    "norm_f='backbone.norm_f.weight'\n",
    "lm_head='lm_head.weight'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Generate a single token for multiple token inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "nsamples=1\n",
    "send_ssm=[]\n",
    "send_conv=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def selective_scan(layer_idx,u, delta, A, B, C, D=None, z=None, delta_bias=None, delta_softplus=False,\n",
    "                      return_last_state=False):\n",
    "    \"\"\"\n",
    "    u: r(B D L)\n",
    "    delta: r(B D L)\n",
    "    A: c(D N) or r(D N)\n",
    "    B: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)\n",
    "    C: c(D N) or r(B N L) or r(B N 2L) or r(B G N L) or (B G N L)\n",
    "    D: r(D)\n",
    "    z: r(B D L)\n",
    "    delta_bias: r(D), fp32\n",
    "\n",
    "    out: r(B D L)\n",
    "    last_state (optional): r(B D dstate) or c(B D dstate)\n",
    "    \"\"\"\n",
    "    dtype_in = u.dtype\n",
    "    u = u.float()\n",
    "    delta = delta.float()\n",
    "    if delta_bias is not None:\n",
    "        delta = delta + delta_bias[..., None].float()\n",
    "    if delta_softplus:\n",
    "        delta = F.softplus(delta)\n",
    "    batch, dim, dstate = u.shape[0], A.shape[0], A.shape[1]\n",
    "    is_variable_B = B.dim() >= 3\n",
    "    is_variable_C = C.dim() >= 3\n",
    "    if A.is_complex():\n",
    "        if is_variable_B:\n",
    "            B = torch.view_as_complex(rearrange(B.float(), \"... (L two) -> ... L two\", two=2))\n",
    "        if is_variable_C:\n",
    "            C = torch.view_as_complex(rearrange(C.float(), \"... (L two) -> ... L two\", two=2))\n",
    "    else:\n",
    "        B = B.float()\n",
    "        C = C.float()\n",
    "    x = A.new_zeros((batch, dim, dstate))\n",
    "    ys = []\n",
    "    deltaA = torch.exp(torch.einsum('bdl,dn->bdln', delta, A))\n",
    "    if not is_variable_B:\n",
    "        deltaB_u = torch.einsum('bdl,dn,bdl->bdln', delta, B, u)\n",
    "    else:\n",
    "        if B.dim() == 3:\n",
    "            deltaB_u = torch.einsum('bdl,bnl,bdl->bdln', delta, B, u)\n",
    "        else:\n",
    "            B = repeat(B, \"B G N L -> B (G H) N L\", H=dim // B.shape[1])\n",
    "            deltaB_u = torch.einsum('bdl,bdnl,bdl->bdln', delta, B, u)\n",
    "    if is_variable_C and C.dim() == 4:\n",
    "        C = repeat(C, \"B G N L -> B (G H) N L\", H=dim // C.shape[1])\n",
    "    last_state = None\n",
    "    for i in range(u.shape[2]):\n",
    "\n",
    "        x = deltaA[:, :, i] * x + deltaB_u[:, :, i] #x is as same as ssm_state(ht) [b d n]\n",
    "        \n",
    "        if not is_variable_C:\n",
    "            y = torch.einsum('bdn,dn->bd', x, C)\n",
    "        else:\n",
    "            if C.dim() == 3:\n",
    "                y = torch.einsum('bdn,bn->bd', x, C[:, :, i])\n",
    "            else:\n",
    "                y = torch.einsum('bdn,bdn->bd', x, C[:, :, :, i])\n",
    "        if i == u.shape[2] - 1:\n",
    "            last_state = x       \n",
    "\n",
    "        if y.is_complex():\n",
    "            y = y.real * 2\n",
    "        ys.append(y)\n",
    "    y = torch.stack(ys, dim=2) # (batch dim L)\n",
    "    out = y if D is None else y + u * rearrange(D, \"d -> d 1\")\n",
    "    if z is not None:\n",
    "        out = out * F.silu(z)\n",
    "    out = out.to(dtype=dtype_in)\n",
    "\n",
    "    return out if not return_last_state else (out, last_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def model_inference(input_ids,seqlen,sentence):\n",
    "\n",
    "    embedding_layer = nn.Embedding.from_pretrained(model_weights['backbone.embeddings.weight']) #or model_weights['backbone.embedding.weight'] : ~2.8b\n",
    "    embedded_input = embedding_layer(input_ids)\n",
    "    for layer_idx in range(layer_num):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if layer_idx==0:\n",
    "                hidden_state=embedded_input.to(torch.bfloat16)\n",
    "                prev_residual = torch.zeros(nsamples,seqlen,d_model).to(device=\"cuda\").to(torch.bfloat16)\n",
    "                ssm_state=torch.zeros(nsamples,d_model*2,d_state).to(torch.bfloat16).to(torch.bfloat16)\n",
    "                conv_state=torch.zeros(nsamples,d_model*2,d_conv).to(torch.bfloat16)\n",
    "\n",
    "            residual = hidden_state + prev_residual\n",
    "\n",
    "            norm_cls=RMSNorm(hidden_size=d_model).cuda()\n",
    "            norm_weight=model_weights[norm_weights[layer_idx]]\n",
    "            norm_cls.weight = nn.Parameter(norm_weight)\n",
    "            normalized_hidden_input = norm_cls(residual)\n",
    "\n",
    "            xz = normalized_hidden_input @ model_weights[mixer_in_proj_weights[layer_idx]].T\n",
    "    \n",
    "            x, z = xz.chunk(2, dim=2)\n",
    "\n",
    "            conv_state.copy_(nn.functional.pad(rearrange(x,\"b l d -> b d l\"), (d_conv - seqlen, 0)))\n",
    "            send_conv.append(torch.clone(conv_state))\n",
    "        \n",
    "            x = causal_conv1d_fn(\n",
    "                x=rearrange(x, \"b l d -> b d l\"),\n",
    "                weight=rearrange(model_weights[mixer_conv1d_weights[layer_idx]], \"b l d -> (b l) d\"),\n",
    "                bias=model_weights[mixer_conv1d_bias[layer_idx]],\n",
    "                activation=\"silu\",\n",
    "            )\n",
    "\n",
    "            \n",
    "            x = rearrange(x, \"b d l -> b l d\")\n",
    "\n",
    "            x_dbl = x @ model_weights[mixer_x_proj_weights[layer_idx]].T  #[b l 8192] [8192 288]\n",
    "\n",
    "            dt, B, C = torch.split(x_dbl, [dt_rank, d_state, d_state], dim=-1)    #[b l 256]  [b l 16] [b l 16]\n",
    "\n",
    "  \n",
    "            dt = dt @ model_weights[mixer_dt_proj_weights[layer_idx]].T  #[b l 256] [256 8192]\n",
    "\n",
    "            dt = rearrange(dt, \"b l d -> b d l\", l=seqlen)     #[b 8192 l]\n",
    "\n",
    "\n",
    "\n",
    "            B = rearrange(B, \"b l dstate -> b dstate l\", l=seqlen).contiguous()\n",
    "            C = rearrange(C, \"b l dstate -> b dstate l\", l=seqlen).contiguous()\n",
    "\n",
    "\n",
    "            A = -torch.exp(model_weights[mixer_A_log_weights[layer_idx]]).float()\n",
    "\n",
    "            D=model_weights[mixer_D[layer_idx]].float()\n",
    "            \n",
    "            dt_proj_bias=model_weights[mixer_dt_proj_bias[layer_idx]].float()\n",
    "\n",
    "            z=rearrange(z,\"b l d -> b d l\")\n",
    "            x=rearrange(x,\"b l d -> b d l\")\n",
    "\n",
    "\n",
    "            y=selective_scan(\n",
    "                layer_idx,\n",
    "                x,\n",
    "                dt,\n",
    "                A,\n",
    "                B,\n",
    "                C,\n",
    "                D,\n",
    "                z=z,\n",
    "                delta_bias=dt_proj_bias,\n",
    "                delta_softplus=True,\n",
    "                return_last_state=ssm_state is not None,\n",
    "                )\n",
    "\n",
    "            if ssm_state is not None:\n",
    "                y, last_state = y\n",
    "                ssm_state.copy_(last_state)\n",
    "\n",
    "            y= rearrange(y, \"b d l -> b l d\")\n",
    "\n",
    "            out = y @ model_weights[mixer_out_proj_weights[layer_idx]].T #[b l 2d] [2d d]\n",
    "\n",
    "            hidden_state = out\n",
    "            prev_residual = residual\n",
    "\n",
    "            send_ssm.append(torch.clone(ssm_state))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        residual_f=hidden_state+prev_residual\n",
    "        norm_f_weight=model_weights[norm_f]\n",
    "        norm_cls=RMSNorm(hidden_size=d_model).cuda()\n",
    "        norm_cls.weight = nn.Parameter(norm_f_weight)\n",
    "        hidden_state_f = norm_cls(residual_f)\n",
    "\n",
    "        lm_head_weight=model_weights[lm_head]\n",
    "        lm_logit=hidden_state_f @ lm_head_weight.T\n",
    "\n",
    "        scores=[]\n",
    "        scores.append(lm_logit[0][seqlen-1])\n",
    "\n",
    "        token=sample(rearrange(scores[-1],\"d -> 1 d\"),top_k=40,top_p=0.9,temperature=0.9)\n",
    "        gen_word=token.unsqueeze(1)\n",
    "        input=tokenizer.decode(gen_word[0][0])\n",
    "\n",
    "        return lm_logit,input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Generate one token for one token input(Follow-up process from \"Generate a single token for multiple token inputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn\n",
    "def model_gen(input_zero,gen_token_num=1024):\n",
    "    gen_sentence=input_zero\n",
    "    gen_token=input_zero\n",
    "    act=nn.SiLU()\n",
    "    for gen_idx in range(gen_token_num):\n",
    "        with torch.no_grad():\n",
    "            tokens = tokenizer(gen_token, return_tensors=\"pt\")\n",
    "            input_ids = tokens.input_ids.to(device=\"cuda\")\n",
    "            embedding_layer = nn.Embedding.from_pretrained(model_weights['backbone.embeddings.weight'])\n",
    "            embedded_input=embedding_layer(input_ids.to(device=\"cuda\"))\n",
    "            for layer_idx in range(layer_num):\n",
    "                if layer_idx==0:\n",
    "                    hidden_state=embedded_input.to(torch.bfloat16)\n",
    "                    prev_residual = torch.zeros(nsamples,d_model).to(device=\"cuda\").to(torch.bfloat16)\n",
    "\n",
    "                residual = hidden_state + prev_residual\n",
    "\n",
    "                norm_cls=RMSNorm(hidden_size=d_model).cuda()\n",
    "                norm_weight=model_weights[norm_weights[layer_idx]]\n",
    "                norm_cls.weight = nn.Parameter(norm_weight)\n",
    "                normalized_hidden_input = norm_cls(residual)\n",
    "\n",
    "                xz =  normalized_hidden_input.squeeze(1) @ model_weights[mixer_in_proj_weights[layer_idx]].T\n",
    "                x, z = xz.chunk(2, dim=-1)\n",
    "\n",
    "    #----------------------conv1d stage start ----------------------------------------          \n",
    "                conv_state=torch.clone(send_conv[layer_idx])\n",
    "\n",
    "                conv_state.copy_(torch.roll(conv_state.cuda(), shifts=-1, dims=-1))  # Update state (B D W)\n",
    "                conv_state[:, :, -1] = x\n",
    "                x = torch.sum(conv_state.cuda() * rearrange(model_weights[mixer_conv1d_weights[layer_idx]], \"d 1 w -> d w\"), dim=-1)  # (B D)\n",
    "                x = x + model_weights[mixer_conv1d_bias[layer_idx]]\n",
    "                x = act(x)\n",
    "\n",
    "                send_conv[layer_idx] = conv_state\n",
    "    #----------------------conv1d stage end---------------------------------------- \n",
    "                x_dbl = x @ model_weights[mixer_x_proj_weights[layer_idx]].T\n",
    "                dt, B, C = torch.split(x_dbl, [dt_rank, d_state, d_state], dim=-1)\n",
    "            \n",
    "                dt = dt @ model_weights[mixer_dt_proj_weights[layer_idx]].T\n",
    "                A = -torch.exp(model_weights[mixer_A_log_weights[layer_idx]].float())\n",
    "                D=model_weights[mixer_D[layer_idx]]\n",
    "        \n",
    "                dt_proj_bias=model_weights[mixer_dt_proj_bias[layer_idx]]\n",
    "\n",
    "    #----------------------ssm stage start---------------------------------------- \n",
    "                ssm_state=torch.clone(send_ssm[layer_idx])\n",
    "\n",
    "                dt = nn.functional.softplus(dt + dt_proj_bias)\n",
    "                dA = torch.exp(torch.einsum(\"bd,dn->bdn\", dt, A))\n",
    "                dB = torch.einsum(\"bd,bn->bdn\", dt, B)\n",
    "                ssm_state.copy_(ssm_state.cuda() * dA + rearrange(x, \"b d -> b d 1\") * dB)\n",
    "                y = torch.einsum(\"bdn,bn->bd\", ssm_state.cuda(), C)\n",
    "                y = y + D * x\n",
    "                y = y * act(z)  # (B D)\n",
    "\n",
    "                send_ssm[layer_idx] = ssm_state\n",
    "    #----------------------ssm stage end----------------------------------------   \n",
    "\n",
    "\n",
    "    #----------------------out_put----------------------------------------------            \n",
    "                out = y @ model_weights[mixer_out_proj_weights[layer_idx]].T\n",
    "\n",
    "                hidden_state = out\n",
    "                prev_residual = residual\n",
    "\n",
    "\n",
    "    #------------- don't modify ------------------------------------\n",
    "            residual_f=hidden_state+prev_residual\n",
    "            norm_f_weight=model_weights[norm_f]\n",
    "            norm_cls=RMSNorm(hidden_size=d_model).cuda()\n",
    "            norm_cls.weight = nn.Parameter(norm_f_weight)\n",
    "            hidden_state_f = norm_cls(residual_f)\n",
    "\n",
    "            lm_head_weight=model_weights[lm_head]\n",
    "            lm_logit=hidden_state_f @ lm_head_weight.T\n",
    "            token=sample(rearrange(lm_logit[0][0],\"d -> 1 d\"),top_k=40,top_p=0.9,temperature=0.9)\n",
    "            gen_word=token.unsqueeze(1)\n",
    "            gen_token=tokenizer.decode(gen_word[0][0])\n",
    "            gen_sentence += gen_token\n",
    "\n",
    "    return gen_sentence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
